{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "15QfB7RAuXAc"
   },
   "source": [
    "# Neural Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gucid6KNuXAe"
   },
   "source": [
    "In this Exercise, we will be using Pytorch Lightning to implement our neural LM. Your job will be just to write the forward method of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yL_M2zf4myYa"
   },
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRRrn78ZjL54"
   },
   "outputs": [],
   "source": [
    "# #download corpus\n",
    "!wget --no-check-certificate https://github.com/ekapolc/nlp_2019/raw/master/HW4/BEST2010.zip\n",
    "!unzip BEST2010.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGmYebp38OUl"
   },
   "outputs": [],
   "source": [
    "!pip install lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IR4HK5jQm17K"
   },
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPE1RqKOrWJ0"
   },
   "outputs": [],
   "source": [
    "total_word_count = 0\n",
    "best2010 = []\n",
    "with open('BEST2010/news.txt','r',encoding='utf-8') as f:\n",
    "  for i,line in enumerate(f):\n",
    "    line=line.strip()[:-1] #remove the trailing |\n",
    "    total_word_count += len(line.split(\"|\"))\n",
    "    best2010.append(line)\n",
    "\n",
    "train = best2010[:int(len(best2010)*0.7)]\n",
    "test = best2010[int(len(best2010)*0.7):]\n",
    "#Training data\n",
    "train_word_count =0\n",
    "for line in train:\n",
    "    for word in line.split('|'):\n",
    "        train_word_count+=1\n",
    "print ('Total sentences in BEST2010 news training dataset :\\t'+ str(len(train)))\n",
    "print ('Total word counts in BEST2010 news training dataset :\\t'+ str(train_word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQBjqe5arHGX"
   },
   "source": [
    "Here we are going to use a library from huggingface called `tokenizers`. This will help us create a vocabulary and handle the encoding and decoding, i.e., convert text to its corresponding ID (which will be learned by the tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elwE0gh2rE3C"
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import CharDelimiterSplit\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "#Basically, we just use the new tokenizer as our vocab building tool.\n",
    "#In practice, you will have to use a compatible tokenizer like newmm to tokenize the corpus first then do this step\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = CharDelimiterSplit(delimiter=\"|\") #now the tokenizer will split \"|\" for us\n",
    "trainer = WordLevelTrainer(min_frequency=3,  #we can set a frequency threshold for taking a word into our vocab. for this example, words with freq < 3 will be excluded from the vocab.\n",
    "                           special_tokens=[\"[UNK]\", \"<s>\", \"</s>\"]) #these are our special tokens: for unknown, begin-of-sentence, and end-of-sentence, respectively.\n",
    "tokenizer.train_from_iterator(train, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrKtjv4PJpg2"
   },
   "outputs": [],
   "source": [
    "len(tokenizer.get_vocab()) #same as nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqM_jrZwrJpB"
   },
   "outputs": [],
   "source": [
    "tokenizer.encode(\"กฎหมาย|กับ|การ|เบียดบัง|คน|จน|asdf\").tokens #tokens we get after tokenizing this sentence. unknown words will be tokenized as [UNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1r1pJ1B_sp9j"
   },
   "outputs": [],
   "source": [
    "tokenizer.encode(\"กฎหมาย|กับ|การ|เบียดบัง|คน|จน|asdf\").ids #this is what we will feed to the LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fkx6CSoXWXmG"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import lightning as L\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XHJsP8_898x"
   },
   "outputs": [],
   "source": [
    "L.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-r_kyrrrDHZq"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "  def __init__(self, data, seq_len = 128):\n",
    "    #  data is currently a list of sentences\n",
    "    #  [sent1,\n",
    "    #   sent2,\n",
    "    #   ...,\n",
    "    #  ]\n",
    "\n",
    "    data = [d+'|</s>' for d in data] #append an </s> token to each sentence\n",
    "    encodings = tokenizer.encode_batch(data) #encode (turn token into token_id) data\n",
    "    token_ids = [enc.ids for enc in encodings] #get the token ids for each sentence\n",
    "    flatten_token_ids = list(itertools.chain(*token_ids)) #turn a list of token_ids into one long token_ids\n",
    "    ## now data looks like this [sent1_ids </s> sent2_ids </s> ...]\n",
    "    encoded = torch.LongTensor(flatten_token_ids)\n",
    "\n",
    "    #remove some left over tokens so that we can form batches of seq_len (128 in this case). Optionally, we can use padding tokens instead.\n",
    "    left_over = len(encoded) % seq_len\n",
    "    encoded = encoded[:len(encoded)-left_over]\n",
    "    self.encoded = encoded.view(-1, seq_len) #reshape data so it becomes a 2-D matrix of shape (len(encoded)//128, 128), i.e. each row contains data of len==128\n",
    "    ## now data looks like this\n",
    "    ## [ [1,2,3, ... , 128] (this is just an example, not actual input_ids)\n",
    "    ##   [1,2,3, ... , 128]\n",
    "    ##   [1,2,3, ... , 128]\n",
    "    ## ]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.encoded[idx]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmW-K0XBZ4Dq"
   },
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "test_batch_size = 128\n",
    "train_dataset = TextDataset(train)\n",
    "train_loader = DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True) #DataLoader will take care of the random sampling and batching of data\n",
    "\n",
    "test_dataset = TextDataset(test)\n",
    "test_loader = DataLoader(test_dataset, batch_size = test_batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElhZcB94MUtC"
   },
   "source": [
    "## Model : Implement the forward function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKNJAolug-1I"
   },
   "outputs": [],
   "source": [
    "class LSTM(L.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, learning_rate, criterion):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) #this will turn the token ids into vectors\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size) #turn the vectors back into token ids\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        embedded = self.dropout(embedded)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        out = self.fc(lstm_out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        src = batch[:, :-1]\n",
    "        target = batch[:, 1:]\n",
    "        prediction = self(src) # run the sequence through the model (the forward method)\n",
    "        prediction = prediction.reshape(-1, vocab_size)\n",
    "        target = target.reshape(-1)\n",
    "        loss = self.criterion(prediction, target)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        src = batch[:, :-1]  #[batch_size (64) , seq_len-1 (127)] except last words\n",
    "        target = batch[:, 1:] #[batch_size (64) , seq_len-1 (127)] except first words\n",
    "        with torch.no_grad(): #disable gradient calculation for faster inference\n",
    "          prediction = self(src) #[batch_size (64), seq_len-1 (127) , vocab size (9000)]\n",
    "        prediction = prediction.reshape(-1, vocab_size) #[batch_size*(seq_len-1) (64*127=8128) , vocab]\n",
    "        target = target.reshape(-1) #[batch_size (64), seq_len-1 (127)] -> [batch_size*(seq_len-1) (8128)]\n",
    "        loss = self.criterion(prediction, target)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBnYCh-miOEr"
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "embedding_dim = 200\n",
    "hidden_dim = 512\n",
    "num_layers = 3\n",
    "dropout_rate = 0.2\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHWXaPsvigPq"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yNEZ4jwXumR"
   },
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import CSVLogger\n",
    "csv_logger = CSVLogger(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZwqhWicMdH0"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kr0zdeMAjD1U"
   },
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=20,\n",
    "    logger=csv_logger,\n",
    "    deterministic=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9qcwNA0mN6J"
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloaders=train_loader) # takes about 8 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUfWF_V6Me9H"
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXVj9ewNqweZ"
   },
   "outputs": [],
   "source": [
    "test_result = trainer.test(model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pVjEyYDtnc-"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuIPToGQs-ZG"
   },
   "outputs": [],
   "source": [
    "print(f\"Perplexity : {np.exp(test_result[0]['test_loss'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAZwiRqsnOPe"
   },
   "outputs": [],
   "source": [
    "model.eval() #disable dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFtebDAmVh_T"
   },
   "outputs": [],
   "source": [
    "unk_token_id = tokenizer.encode(\"[UNK]\").ids\n",
    "eos_token_id = tokenizer.encode(\"</s>\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hj-V4OsDqpBO"
   },
   "outputs": [],
   "source": [
    "def generate_seq(context, max_new_token = 10):\n",
    "  encoded = tokenizer.encode(context).ids\n",
    "  with torch.no_grad():\n",
    "      for i in range(max_new_token):\n",
    "          src = torch.LongTensor([encoded]).to(model.device)\n",
    "          prediction = model(src)\n",
    "          probs = torch.softmax(prediction[:, -1] / 1, dim=-1)\n",
    "          prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "          while prediction == unk_token_id:\n",
    "              prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "          if prediction == eos_token_id:\n",
    "              break\n",
    "\n",
    "          encoded.append(prediction)\n",
    "\n",
    "  return tokenizer.decode(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u20r9w8zvJi4"
   },
   "outputs": [],
   "source": [
    "context = \"<s>|วัน|จันทร์\"\n",
    "generate_seq(context, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fr536NVvGX3"
   },
   "source": [
    "## Questions: Answer the following in MyCourseville\n",
    "\n",
    "1. What is the perplexity of the neural LM you trained?\n",
    "2. Paste your favorite sentence generated with the LM."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1sCWXCiBDs6UhYtxXTfWfIb30nC7kHTgc",
     "timestamp": 1612276273852
    },
    {
     "file_id": "1FIsDx7KTE5tiF-Xag22pl4VLQZc6G8Yw",
     "timestamp": 1612057948373
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
